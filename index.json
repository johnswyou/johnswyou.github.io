[{"authors":null,"categories":null,"content":"I am a masters student at the University of Waterloo’s Civil and Environmental Engineering department under the supervision of professor John Quilty. My research analyzes the joint usage of wavelet decomposition and machine learning for stream flow forecasting of snow-melt driven basins across Canada. I love meeting new people and learning about everything, feel free to message me!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://johnswyou.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a masters student at the University of Waterloo’s Civil and Environmental Engineering department under the supervision of professor John Quilty. My research analyzes the joint usage of wavelet decomposition and machine learning for stream flow forecasting of snow-melt driven basins across Canada.","tags":null,"title":"John You","type":"authors"},{"authors":null,"categories":null,"content":"Introduction (optional read) The extreme learning machine (ELM) model is a machine learning (ML) technique that can be applied to both regression and classification problems. ELM is a feed-forward neural network with one hidden layer. The most important difference between a regular feed-forward neural network with one hidden layer and ELM is that ELM does not use backpropagation to learn its weights and biases. Instead, all weights and biases that map input vectors to the single hidden layer are assigned random values, and the weights that map the hidden layer to the output (a predicted target in the case of regression) are learned via least squared, not unlike ordinary least squares (OLS) linear regression. In fact, we will see shortly that the equation for ELM and OLS have the exact same form. It should be clear that the main advantage of ELM is reduced training time relative to training a neural network with the same architecture using backpropagation. One disadvantage of ELM is the questionable practice of assigning random values to the weights and biases that map input vectors to the single hidden layer. It is unclear how this affects the generalizability of ELM.\nELM was popularized by a 2006 paper titled “Extreme learning machine: Theory and applications” by Huang, Zhu and Siew. ELM has been a controversial topic, and while I won’t go into details here, you can start with this great article if you want to dig into the drama.\nELM algorithm in math Given a training set $D=\\left\\{\\left(\\mathbf{x}_i, y_i\\right) \\mid \\mathbf{x}_i \\in \\mathbf{R}^p, y_i \\in \\mathbf{R}, i=1, \\ldots, N\\right\\}$, activation function $g(x)$, and hidden node number $L$,\nStep 1: Randomly assign input weight $\\mathbf{w}_i$ and bias $b_i, i=1, \\ldots, L$. Step 2: Calculate the hidden layer output matrix $\\mathbf{H}$. Step 3: Calculate the output weight $\\beta=\\mathbf{H}^{\\dagger} \\mathbf{y}$, where $\\mathbf{y}=\\left[y_1, \\ldots, y_N\\right]^{\\mathrm{T}}$ and $\\dagger$ indicates the Moore-Penrose Pseudo-inverse. The hidden layer output matrix looks as follows:\n$$ \\mathbf{H}=\\left[\\begin{array}{ccc}g\\left(\\mathbf{w}_1 \\cdot \\mathbf{x}_1+b_1\\right) \u0026amp; \\cdots \u0026amp; g\\left(\\mathbf{w}_{L} \\cdot \\mathbf{x}_1+b_{L}\\right) \\\\ \\vdots \u0026amp; \\cdots \u0026amp; \\vdots \\\\ g\\left(\\mathbf{w}_1 \\cdot \\mathbf{x}_N+b_1\\right) \u0026amp; \\cdots \u0026amp; g\\left(\\mathbf{w}_{L} \\cdot \\mathbf{x}_N+b_{L}\\right)\\end{array}\\right]_{N \\times L}. $$ Notice that each row in $\\mathbf{H}$ corresponds to the hidden layer output for sample $\\mathbf{x}_i$.\nIf we collect the hidden layer weights into a matrix $\\mathbf{W}=\\left[\\mathbf{w}_1, \\cdots, \\mathbf{w}_L\\right]_{p \\times L}$, hidden layer biases into a vector $\\mathbf{b} = [b_1, \\cdots, b_L]^\\mathrm{T}$, and input features $\\{\\mathbf{x}_i\\}$ into an $N \\times p$ matrix $\\mathbf{X} = [\\mathbf{x}_1, \\cdots, \\mathbf{x}_N]^\\mathrm{T}$, we can compute $\\mathbf{H}$ in one go by\n$$ \\mathbf{H} = g\\left(\\mathbf{X}\\mathbf{W}+\\mathbf{b}\\right) $$ where $g(\\cdot)$ is applied element-wise.\nELM algorithm in code import numpy as np # g(x) def sigmoid_activation(x): return 1/(1+np.exp(-x)) # Create a dummy dataset p = 5 # Number of input features N = 10000 # Numver of observations X = np.random.random((N, p)) Y = np.random.random(N) L = 10 # Number of hidden nodes # ELM code; 3 lines W, b = np.random.random((p, L)), np.random.random(L) # Step 1: Weight matrix, bias vector H = sigmoid_activation((X @ W) + b) # Step 2: Hidden layer output matrix beta_hat = np.linalg.pinv(H) @ Y # Step 3: Output weights Here is what the output weight vector looks like:\nbeta_hat array([-0.49835507, -0.68508437, -0.12280456, 0.37581779, -0.04470077, 3.09404425, -1.1261844 , -0.89855811, -1.14341015, 1.46157009]) How do we make predictions? At inference time, the same weight matrix and bias vector that was randomly generated at training time must be used. This is a key implementation detail. Otherwise, prediction is straight-forward:\nStep 1: Take new input features $\\mathbf{X}_{new}$ and compute $\\mathbf{H}_{new}$ using the same $\\mathbf{W}$ and $\\mathbf{b}$ generated at training time. Step 2: Compute predictions $\\mathbf{\\hat{y}}=\\mathbf{H}_{new}\\mathbf{\\hat{\\beta}}$, where $\\mathbf{\\hat{\\beta}}$ is the output weight vector learned at training time. Application: California housing data set (optional read) If you’re well versed in the Python library Scikit-Learn, here I implement a custom sklearn regression estimator and use it to perform a grid search over various hidden layer sizes. I use the California housing data set, which is provided in sklearn. The California housing data set is a classic toy data set.\nfrom sklearn.datasets import fetch_california_housing X, y = fetch_california_housing(return_X_y=True) Let’s analyze the effect of L (number of nodes in the hidden layer) on prediction performance. We will define prediction performance as the coefficient of determination (R squared) evaluated using 10 fold cross validation.\nFirst, we will define a custom sklearn regression …","date":1676073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676073600,"objectID":"218b5fadaf14c209d1ba3de96efe699a","permalink":"https://johnswyou.github.io/post/elm-in-3-lines/","publishdate":"2023-02-11T00:00:00Z","relpermalink":"/post/elm-in-3-lines/","section":"post","summary":"Introduction (optional read) The extreme learning machine (ELM) model is a machine learning (ML) technique that can be applied to both regression and classification problems. ELM is a feed-forward neural network with one hidden layer.","tags":null,"title":"Extreme Learning Machine in 3 Lines of Code","type":"post"},{"authors":["John You"],"categories":[],"content":"","date":1675121980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675121980,"objectID":"dcf90c0454ea820bbd1c5aee311fdd59","permalink":"https://johnswyou.github.io/project/py_wddff/","publishdate":"2023-01-30T18:39:40-05:00","relpermalink":"/project/py_wddff/","section":"project","summary":"Wavelet data driven forecasting framework (WDDFF) implementation in Python 3.","tags":"Wavelets","title":"py_wddff","type":"project"},{"authors":["John Quilty","John You"],"categories":[],"content":"","date":1675121446,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675121446,"objectID":"b3850f6ae40a350e1c182babeb80ef3b","permalink":"https://johnswyou.github.io/project/hydroivs/","publishdate":"2023-01-30T18:30:46-05:00","relpermalink":"/project/hydroivs/","section":"project","summary":"Several Innput Variable Selection (IVS) methods for forecasting research in water resources.","tags":["Feature Selection"],"title":"HydroIVS","type":"project"},{"authors":["John Quilty","John You"],"categories":[],"content":"","date":1675120532,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675120532,"objectID":"ac7d1d67ad71621776db27871d645d01","permalink":"https://johnswyou.github.io/project/fastwavelets/","publishdate":"2023-01-30T18:15:32-05:00","relpermalink":"/project/fastwavelets/","section":"project","summary":"Compute Maximal Overlap Discrete Wavelet Transform (DWT) and A Trous DWT.","tags":["Wavelets"],"title":"FastWavelets","type":"project"}]