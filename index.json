[{"authors":null,"categories":null,"content":"I am a masters student at the University of Waterloo’s Civil and Environmental Engineering department under the supervision of professor John Quilty. My research analyzes the joint usage of wavelet decomposition and machine learning for stream flow forecasting of snow-melt driven basins across Canada. I love meeting new people and learning about everything, feel free to message me!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://johnswyou.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a masters student at the University of Waterloo’s Civil and Environmental Engineering department under the supervision of professor John Quilty. My research analyzes the joint usage of wavelet decomposition and machine learning for stream flow forecasting of snow-melt driven basins across Canada.","tags":null,"title":"John You","type":"authors"},{"authors":null,"categories":null,"content":"In this article, I show the mechanics of the discrete Fourier transform (DFT) and circular convolution (within the context of time series analysis). The goal of this post is to present these two methods in a practical yet non-superficial way. We will only deal with real, finite length time series $\\left\\{a_t\\right\\}$. The primary reference for this article is “Wavelet Methods for Time Series Analysis” by Percival and Walden.\nDiscrete Fourier Transform (DFT) The discrete Fourier transform takes a time series from the time domain to the frequency domain. The transform allows us to see the frequency content of a time series.\nSuppose we are given a time series\n$$ \\left\\{a_t\\right\\}=\\left\\{a_t: t=0, \\ldots, N-1\\right\\} $$ Its discrete Fourier transform is the complex sequence $\\left\\{A_k\\right\\}$ given by:\n$$ A_k \\equiv \\sum_{t=0}^{N-1} a_t e^{-i 2 \\pi t k / N}, \\quad k=0, \\ldots, N-1 $$ Note that $A_k$ is associated with frequency $f_k \\equiv k / N$.\nimport numpy as np from matplotlib import pyplot as plt def naive_finite_fourier_transform(a): # a can be a list or 1D numpy array N = len(a) A = [] for k in range(N): A_k = 0 for t in range(N): A_k += a[t] * np.exp((-1j * 2 * np.pi * t * k)/N) A.append(A_k) A = np.asarray(A) f = [k/N for k in range(N)] f = np.asarray(f) return A, f We can reconstruct $\\left\\{a_t\\right\\}$ from its DFT $\\left\\{A_k\\right\\}$ using the inverse DFT:\n$$ \\frac{1}{N} \\sum_{k=0}^{N-1} A_k e^{i 2 \\pi t k / N}=a_t, \\quad t=0, \\ldots, N-1 $$ def naive_inverse_finite_fourier_transform(A): # a can be a list or 1D numpy array N = len(A) a = [] for t in range(N): a_t = 0 for k in range(N): a_t += A[k] * np.exp((1j * 2 * np.pi * t * k)/N) a.append((1/N)*a_t) a = np.asarray(a) t = list(range(N)) t = np.asarray(t) return np.real(a), t Example a = np.random.random(100) A, f = naive_finite_fourier_transform(a) plt.plot(f[1:], np.abs(A[1:])**2) plt.axvline(x=0.5, color=\u0026#34;red\u0026#34;, linestyle = \u0026#34;--\u0026#34;) plt.show() a1, _ = naive_inverse_finite_fourier_transform(A) np.allclose(a, a1) True Circular Convolution/Filtering Circular convolution (also called circular filtering) is a simple idea, but is cumbersome to read when described with math. For this reason, I will first show how circular convolution works using pictures before presenting any mathematics.\nSuppose we have two time series, $\\{a_t\\}$ and $\\{b_t\\}$, both of length $N=10$. I won’t specify the value of the time series here (I’ll leave them to the reader’s imagination). I will only show the time indices of each time series in the pictures below. The indices of the first time series $\\{a_t\\}$ will be shown in red as follows:\nand the indices of the second series $\\{b_t\\}$ will be shown in black to distinguish it from $\\{a_t\\}$:\nThe first step is to reverse the order of $\\{a_t\\}$, which makes its indices look as follows:\nNext, we concatenate $\\{b_t\\}$ front to back and perform a sequence of sliding dot products between $\\{a_t\\}$ and the newly lengthened version of $\\{b_t\\}$ as follows:\nThe above picture shows how the first element of the result of a circular convolution between $\\{a_t\\}$ and $\\{b_t\\}$ is constructed:\nThe next element can be constructed as follows:\nand here’s element $N-2$ of the resultant circular convolution time series:\nIn general, the result of a convolution between two time series (both of length $N$) is a resultant time series also of length $N$.\nMathematically, the $t^{th}$ element of the resultant time series from a circular convolution between two time series $\\{a_t\\}$ and $\\{b_t\\}$ (both of length $N$) is:\n$$ a * b_t \\equiv \\sum_{u=0}^{N-1} a_u b_{t-u} $$ where $t = 0, 1, \\dots, N-1$.\nWhenever the subscript on $b$ is negative (which occurs whenever $u$ is greater than $t$), we define $b_{-1}=b_{N-1} ; b_{-2}=b_{N-2}$ and so on.\nMore compactly (at the expense of readability), we can also write the $t^{th}$ element of the resultant time series of a circular convolution between two time series $\\{a_t\\}$ and $\\{b_t\\}$ as:\n$$ a * b_t \\equiv \\sum_{u=0}^{N-1} a_u b_{(t-u) \\bmod N}, \\quad t=0, \\ldots, N-1 $$ where $\\bmod$ is the modulo operator.\nHere’s a coded implementation:\ndef naive_circular_convolution(a, b): assert len(a) == len(b) N = len(a) c = [] for t in range(N): c_t = 0 for u in range(N): c_t += a[u]*b[(t-u) % N] c.append(c_t) return np.asarray(c) Circular Convolution Using DFT A key fact is that a circular convolution can be computed using the DFT and inverse DFT. Here’s how:\nCompute the DFT of $\\{a_t\\}$, denoted by $\\{A_k\\}$ Compute the DFT of $\\{b_t\\}$, denoted by $\\{B_k\\}$ Take the inverse DFT of $\\{A_{k} B_{k}\\}$ def naive_circular_convolution_dft(a, b): A, _ = naive_finite_fourier_transform(a) # STEP 1 B, _ = naive_finite_fourier_transform(b) # STEP 2 # STEP 3 C = A*B c, _ = naive_inverse_finite_fourier_transform(C) return c Example a = np.random.random(100) b = np.random.random(100) c = naive_circular_convolution(a, b) c1 = naive_circular_convolution_dft(a, b) np.allclose(c, c1) True Final Words An interesting point to …","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677628800,"objectID":"b3d70adc0c9bf82121fa381ea7cf6b00","permalink":"https://johnswyou.github.io/post/dft-circular-convolution/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/post/dft-circular-convolution/","section":"post","summary":"In this article, I show the mechanics of the discrete Fourier transform (DFT) and circular convolution (within the context of time series analysis). The goal of this post is to present these two methods in a practical yet non-superficial way.","tags":null,"title":"Circular Convolution Using Discrete Fourier Transform","type":"post"},{"authors":null,"categories":null,"content":"Introduction (optional read) The extreme learning machine (ELM) model is a machine learning (ML) technique that can be applied to both regression and classification problems. ELM is a feed-forward neural network with one hidden layer. The most important difference between a regular feed-forward neural network with one hidden layer and ELM is that ELM does not use backpropagation to learn its weights and biases. Instead, all weights and biases that map input vectors to the single hidden layer are assigned random values, and the weights that map the hidden layer to the output (a predicted target in the case of regression) are learned via least squared, not unlike ordinary least squares (OLS) linear regression. In fact, we will see shortly that the equation for ELM and OLS have the exact same form. It should be clear that the main advantage of ELM is reduced training time relative to training a neural network with the same architecture using backpropagation. One disadvantage of ELM is the questionable practice of assigning random values to the weights and biases that map input vectors to the single hidden layer. It is unclear how this affects the generalizability of ELM.\nELM was popularized by a 2006 paper titled “Extreme learning machine: Theory and applications” by Huang, Zhu and Siew. ELM has been a controversial topic, and while I won’t go into details here, you can start with this great article if you want to dig into the drama.\nELM algorithm in math Given a training set $D=\\left\\{\\left(\\mathbf{x}_i, y_i\\right) \\mid \\mathbf{x}_i \\in \\mathbf{R}^p, y_i \\in \\mathbf{R}, i=1, \\ldots, N\\right\\}$, activation function $g(x)$, and hidden node number $L$,\nStep 1: Randomly assign input weight $\\mathbf{w}_i$ and bias $b_i, i=1, \\ldots, L$. Step 2: Calculate the hidden layer output matrix $\\mathbf{H}$. Step 3: Calculate the output weight $\\beta=\\mathbf{H}^{\\dagger} \\mathbf{y}$, where $\\mathbf{y}=\\left[y_1, \\ldots, y_N\\right]^{\\mathrm{T}}$ and $\\dagger$ indicates the Moore-Penrose Pseudo-inverse. The hidden layer output matrix looks as follows:\n$$ \\mathbf{H}=\\left[\\begin{array}{ccc}g\\left(\\mathbf{w}_1 \\cdot \\mathbf{x}_1+b_1\\right) \u0026amp; \\cdots \u0026amp; g\\left(\\mathbf{w}_{L} \\cdot \\mathbf{x}_1+b_{L}\\right) \\\\ \\vdots \u0026amp; \\cdots \u0026amp; \\vdots \\\\ g\\left(\\mathbf{w}_1 \\cdot \\mathbf{x}_N+b_1\\right) \u0026amp; \\cdots \u0026amp; g\\left(\\mathbf{w}_{L} \\cdot \\mathbf{x}_N+b_{L}\\right)\\end{array}\\right]_{N \\times L}. $$ Notice that each row in $\\mathbf{H}$ corresponds to the hidden layer output for sample $\\mathbf{x}_i$.\nIf we collect the hidden layer weights into a matrix $\\mathbf{W}=\\left[\\mathbf{w}_1, \\cdots, \\mathbf{w}_L\\right]_{p \\times L}$, hidden layer biases into a vector $\\mathbf{b} = [b_1, \\cdots, b_L]^\\mathrm{T}$, and input features $\\{\\mathbf{x}_i\\}$ into an $N \\times p$ matrix $\\mathbf{X} = [\\mathbf{x}_1, \\cdots, \\mathbf{x}_N]^\\mathrm{T}$, we can compute $\\mathbf{H}$ in one go by\n$$ \\mathbf{H} = g\\left(\\mathbf{X}\\mathbf{W}+\\mathbf{b}\\right) $$ where $g(\\cdot)$ is applied element-wise.\nELM algorithm in code import numpy as np # g(x) def sigmoid_activation(x): return 1/(1+np.exp(-x)) # Create a dummy dataset p = 5 # Number of input features N = 10000 # Numver of observations X = np.random.random((N, p)) Y = np.random.random(N) L = 10 # Number of hidden nodes # ELM code; 3 lines W, b = np.random.random((p, L)), np.random.random(L) # Step 1: Weight matrix, bias vector H = sigmoid_activation((X @ W) + b) # Step 2: Hidden layer output matrix beta_hat = np.linalg.pinv(H) @ Y # Step 3: Output weights Here is what the output weight vector looks like:\nbeta_hat array([-0.49835507, -0.68508437, -0.12280456, 0.37581779, -0.04470077, 3.09404425, -1.1261844 , -0.89855811, -1.14341015, 1.46157009]) How do we make predictions? At inference time, the same weight matrix and bias vector that was randomly generated at training time must be used. This is a key implementation detail. Otherwise, prediction is straight-forward:\nStep 1: Take new input features $\\mathbf{X}_{new}$ and compute $\\mathbf{H}_{new}$ using the same $\\mathbf{W}$ and $\\mathbf{b}$ generated at training time. Step 2: Compute predictions $\\mathbf{\\hat{y}}=\\mathbf{H}_{new}\\mathbf{\\hat{\\beta}}$, where $\\mathbf{\\hat{\\beta}}$ is the output weight vector learned at training time. Application: California housing data set (optional read) If you’re well versed in the Python library Scikit-Learn, here I implement a custom sklearn regression estimator and use it to perform a grid search over various hidden layer sizes. I use the California housing data set, which is provided in sklearn. The California housing data set is a classic toy data set.\nfrom sklearn.datasets import fetch_california_housing X, y = fetch_california_housing(return_X_y=True) Let’s analyze the effect of L (number of nodes in the hidden layer) on prediction performance. We will define prediction performance as the coefficient of determination (R squared) evaluated using 10 fold cross validation.\nFirst, we will define a custom sklearn regression …","date":1676073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676073600,"objectID":"218b5fadaf14c209d1ba3de96efe699a","permalink":"https://johnswyou.github.io/post/elm-in-3-lines/","publishdate":"2023-02-11T00:00:00Z","relpermalink":"/post/elm-in-3-lines/","section":"post","summary":"Introduction (optional read) The extreme learning machine (ELM) model is a machine learning (ML) technique that can be applied to both regression and classification problems. ELM is a feed-forward neural network with one hidden layer.","tags":null,"title":"Extreme Learning Machine in 3 Lines of Code","type":"post"},{"authors":["John You"],"categories":[],"content":"","date":1675121980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675121980,"objectID":"dcf90c0454ea820bbd1c5aee311fdd59","permalink":"https://johnswyou.github.io/project/py_wddff/","publishdate":"2023-01-30T18:39:40-05:00","relpermalink":"/project/py_wddff/","section":"project","summary":"Wavelet data driven forecasting framework (WDDFF) implementation in Python 3.","tags":"Wavelets","title":"py_wddff","type":"project"},{"authors":["John Quilty","John You"],"categories":[],"content":"","date":1675121446,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675121446,"objectID":"b3850f6ae40a350e1c182babeb80ef3b","permalink":"https://johnswyou.github.io/project/hydroivs/","publishdate":"2023-01-30T18:30:46-05:00","relpermalink":"/project/hydroivs/","section":"project","summary":"Several Innput Variable Selection (IVS) methods for forecasting research in water resources.","tags":["Feature Selection"],"title":"HydroIVS","type":"project"},{"authors":["John Quilty","John You"],"categories":[],"content":"","date":1675120532,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675120532,"objectID":"ac7d1d67ad71621776db27871d645d01","permalink":"https://johnswyou.github.io/project/fastwavelets/","publishdate":"2023-01-30T18:15:32-05:00","relpermalink":"/project/fastwavelets/","section":"project","summary":"Compute Maximal Overlap Discrete Wavelet Transform (DWT) and A Trous DWT.","tags":["Wavelets"],"title":"FastWavelets","type":"project"}]